---
title: "PhenoScreen: A Dual-Space Contrastive Learning Framework-based Phenotypic Screening Method by Linking Chemical Perturbations to Cellular Morphology"
collection: talks
type: "Talk"
permalink: /talks/iHuman_9th-2024
venue: "The 9th iHmuan Forum 2023"
date: 2024-11-08
location: "Shanghai, China"
---

_Phenotypic drug discovery (PDD) screens compounds in cellular models that represent disease-relevant phenotypes, offering a compelling alternative to traditional target-based approaches. Unlike conventional methods, where compounds act on a single predefined target, PDD identifies compounds capable of exerting therapeutic effects through multiple targets and mechanisms. This makes PDD particularly valuable for discovering first-in-class drugs, especially for diseases with poorly understood molecular mechanisms or those lacking validated therapeutic targets. By enabling broader exploration of biological systems and uncovering multi-target drugs (polypharmacology), PDD provides a powerful strategy for tackling complex diseases. In this study, we introduce PhenoScreen, an AI-driven deep learning framework designed to advance PDD by utilizing large-scale compound-phenotype association data. Through contrastive learning, PhenoScreen connects chemical space with cellular morphological profiles, allowing for accurate prediction of compound-induced phenotypic changes. The model was validated across multiple screening tasks and successfully predicted active compounds inducing user-specified phenotypes with varying inhibitory effects in the osteosarcoma phenotypic model. Moreover, PhenoScreen demonstrated strong generalization to other tumor cell types, rhabdomyosarcoma evaluated in this study, suggesting its ability to capture key phenotypic features shared across cancer cells. These results underscore PhenoScreenâ€™s potential to accelerate drug discovery by identifying novel therapeutic pathways and increasing the diversity of viable drug candidates. [See here](https://book.yunzhan365.com/ydxok/wmvq/mobile/index.html) to learn more._     

![photo](https://github.com/user-attachments/assets/3304e200-28fa-4673-b9ab-41c42c30c61d)

![poster_01](https://github.com/user-attachments/assets/bf198999-f428-4979-ad60-4692b9419e98)


